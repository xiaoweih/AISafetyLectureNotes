"""
Evaluate Defence Ability - Test Your Model Against Reference Attacks
Author: Lingfang Li
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
from torchvision import transforms
import argparse

from Reference_attacks import fgsm_attack, pgd5_attack, pgd20_attack

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        x = F.relu(x)
        x = self.fc4(x)
        output = F.log_softmax(x, dim=1)
        return output

def eval_adv_test(model, device, test_loader, attack_method):
    model.eval()
    correct = 0

    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        data = data.view(data.size(0), 28*28)

        # Generate adversarial examples (needs gradients)
        adv_data = attack_method(model, data, target, device=device)

        # Evaluate on adversarial examples (no gradients needed)
        with torch.no_grad():
            output = model(adv_data)
            pred = output.max(1, keepdim=True)[1]
            correct += pred.eq(target.view_as(pred)).sum().item()

    return correct / len(test_loader.dataset)

def main():

    ################################################################################################
    ## Step 1: Specify your model file (generated by Competition.py)
    ################################################################################################

    your_model_file = "1000.pt"  # change this to your model path

    ################################################################################################
    ## Step 2: Reference attacks to test against
    ## Default: FGSM, PGD-5, PGD-20 (fast attacks, ~8 seconds total on CPU)
    ################################################################################################

    test_attacks = [
        ('FGSM', fgsm_attack),
        ('PGD-5', pgd5_attack),
        ('PGD-20', pgd20_attack),
    ]

    # Optional: Define your own custom attack method
    # def my_custom_attack(model, X, y, device):
    #     """
    #     Your attack implementation here
    #     Args:
    #         model: the model to attack
    #         X: input data [batch_size, 784]
    #         y: true labels [batch_size]
    #         device: torch.device
    #     Returns:
    #         X_adv: adversarial examples [batch_size, 784]
    #     """
    #     # Example: add random noise
    #     epsilon = 0.1
    #     noise = torch.FloatTensor(*X.shape).uniform_(-epsilon, epsilon).to(device)
    #     X_adv = torch.clamp(X + noise, 0.0, 1.0)
    #     return X_adv
    #
    # test_attacks.append(('MyAttack', my_custom_attack))

    ################################################################################################
    ## End of configuration
    ################################################################################################

    parser = argparse.ArgumentParser(description='Defence Evaluation')
    parser.add_argument('--batch-size', type=int, default=128, metavar='N')
    parser.add_argument('--no-cuda', action='store_true', default=False)
    parser.add_argument('--seed', type=int, default=1, metavar='S')
    args = parser.parse_args(args=[])

    use_cuda = not args.no_cuda and torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    torch.manual_seed(args.seed)

    print("="*80)
    print("COMP219 Assignment - Defence Evaluation")
    print("="*80)
    print(f"Model: {your_model_file}")
    print(f"Device: {device}")
    print(f"Testing against {len(test_attacks)} attack(s)")
    print("="*80)

    # load test data
    test_set = torchvision.datasets.FashionMNIST(
        root='data', train=False, download=True,
        transform=transforms.Compose([transforms.ToTensor()])
    )
    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False)

    # load model
    model = Net().to(device)
    model.load_state_dict(torch.load(your_model_file, map_location=device))

    # build defence matrix: [1 model] x [N attacks]
    num_attacks = len(test_attacks)
    defence_matrix = np.zeros(num_attacks)

    print("\nEvaluating...")
    for i, (attack_name, attack_fn) in enumerate(test_attacks):
        print(f"  [{i+1}/{num_attacks}] {attack_name}...", end=" ", flush=True)
        robust_acc = eval_adv_test(model, device, test_loader, attack_fn)
        defence_matrix[i] = robust_acc
        print(f"{robust_acc*100:.2f}%")

    # calculate defence score
    defence_score = np.mean(defence_matrix)

    # display results
    print("\n" + "="*80)
    print("DEFENCE MATRIX (Your Model vs Reference Attacks)")
    print("="*80)
    print(f"\n{'Attack':<15} {'Robust Accuracy'}")
    print("-"*80)
    for i, (attack_name, _) in enumerate(test_attacks):
        print(f"{attack_name:<15} {defence_matrix[i]*100:>6.2f}%")
    print("-"*80)
    print(f"{'Mean':<15} {defence_score*100:>6.2f}%")
    print("="*80)

    print("\n" + "="*80)
    print("DEFENCE SCORE")
    print("="*80)
    print(f"Your Defence Score: {defence_score:.4f}")
    print("="*80)
    print("\nNote:")
    print("  - Higher score is better (more robust model)")
    print("  - Final grading will normalize scores across ALL students")
    print("  - This self-test uses reference attacks only")

if __name__ == '__main__':
    main()
