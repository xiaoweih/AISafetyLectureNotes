# COMP219 Assignment - Baseline Testing Guide

## Setup

### 1. Download Files from GitHub

Download the following files from the GitHub repository:
- `Competition.py` - Your submission template
- `Reference_attacks.py` - Reference attack methods (for testing only)
- `Evaluate_attack.py` - Attack score evaluation script
- `Evaluate_defence.py` - Defence score evaluation script
- `README_baseline.md` - This guide

### 2. Download Reference Models from Canvas

1. Go to Canvas → COMP219 Module → Lecture 14
2. Download `Defenders.zip`
3. Extract to `./Defenders` folder in Competition directory

---

## File Structure

```
Competition/
├── Competition.py              # Your submission file
├── Reference_attacks.py        # Baseline attacks (for testing only)
├── Evaluate_defence.py         # Test defence score
├── Evaluate_attack.py          # Test attack score
└── Defenders/                  # Reference models (download from Canvas - see Setup)
    ├── 0.pt
    ├── 1.pt
    └── ... (10 models total)
```

**Note**: The `Defenders/` folder is **not included in GitHub**. You must download `Defenders.zip` from Canvas → COMP219 Module → Lecture 14, then extract it to the Competition directory.

---

## Usage

### 1. Test Your Attack Score

```bash
python Evaluate_attack.py
```

**Configure** in `Evaluate_attack.py`:

```python
# Step 1: Set Defenders path
defenders_path = "./Defenders"

# Step 2: Define your attack (copy from Competition.py)
def your_attack(model, X, y, device):
    # Your attack implementation here
    return X_adv
```

**Output**:
```
ATTACK MATRIX
Model          Robust Accuracy      Attack Score (1/acc)
0.pt                 52.30%              1.91
1.pt                 48.15%              2.08
...
Mean                 49.82%              2.15

ATTACK SCORE
Your Attack Score: 2.1523

Note:
  - Higher score is better (stronger attack)
  - Final grading will normalize scores across ALL students
```

---

### 2. Test Your Defence Score

```bash
python Evaluate_defence.py
```

**Configure** in `Evaluate_defence.py`:

```python
# Step 1: Set your model file (generated by Competition.py)
your_model_file = "1000.pt"

# Step 2: Reference attacks (default setup)
test_attacks = [
    ('FGSM', fgsm_attack),
    ('PGD-5', pgd5_attack),
    ('PGD-20', pgd20_attack),
]

# Optional: Add your own custom attack (see comments in file)
```

**Output**:
```
DEFENCE MATRIX
Attack          Robust Accuracy
FGSM             45.23%
PGD-5            38.67%
PGD-20           32.15%
Mean             38.68%

DEFENCE SCORE
Your Defence Score: 0.3868

Note:
  - Higher score is better (more robust model)
  - Final grading will normalize scores across ALL students
```

---

## Understanding Scores

### Attack Score
- Calculated as: mean of `1/robust_accuracy` of your attack against reference models
- **Higher is better** (stronger attack)
- Lower robust accuracy → stronger attack → higher score

### Defence Score
- Calculated as: mean robust accuracy of your model against reference attacks
- **Higher is better** (more robust model)

### Final Grading
- Your self-test score is for reference only
- Final grading will normalize scores using **all students' scores**
- This ensures fair comparison across the class

---

## Reference Attacks

Three baseline attacks are provided **for testing purposes only**:

| Attack  | Description |
|---------|-------------|
| FGSM    | Fast one-step attack |
| PGD-5   | 5-step iterative attack |
| PGD-20  | 20-step iterative attack |

### ⚠️ IMPORTANT

- These baselines are **for evaluating your defence only**
- **DO NOT directly copy these methods for your submission**
- Simply using these attacks **will NOT give you good marks**
- You must **develop your own attack methods** based on:
  - Course materials (Lab 7.1 and 7.2)
  - Lecture content
  - Your own research and improvements

**Develop your own methods!** These baselines help you test, not solve the assignment.

---

## Recommended Workflow

1. **Develop** your attack/defence in `Competition.py`
2. **Train** your model and generate `.pt` file using `Competition.py`
3. **Test** attack: `python Evaluate_attack.py`
4. **Test** defence: `python Evaluate_defence.py`
5. **Iterate** based on scores
6. **Verify** epsilon constraint < 0.11 using `p_distance` in `Competition.py`
7. **Submit** your improved `Competition.py`

---

## Troubleshooting

**"Defenders folder not found"**
- Download and extract `Defenders.zip` from Canvas Lecture 14

**"Model file not found"**
- Check path in `evaluate_defence.py`
- Train model first with `Competition.py`

**"Epsilon constraint violated"**
- Check `p_distance` output in `Competition.py`
- Must be < 0.11

**"RuntimeError: element 0 of tensors does not require grad"**
- This has been fixed in the latest version
- Make sure you're using the updated evaluation scripts

---

Questions? Ask in Canvas discussion or lab sessions.
